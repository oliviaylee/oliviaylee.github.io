---
permalink: /
title: "Welcome"
excerpt: "About me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

Hello! I am a Master's student at Stanford University. I completed my B.S. with [Honors](/files/Honors_Thesis.pdf){:target="_blank"} in [Symbolic Systems](https://symsys.stanford.edu/) and a minor in Mathematics, and am currently pursuing a coterminal M.S. in Computer Science. I conduct research with Stanford's [IPRL Lab](https://iprl.stanford.edu/), affiliated with the [Stanford Artificial Intelligence Laboratory (SAIL)](https://ai.stanford.edu/), which seeks to understand the underlying principles of robust sensorimotor coordination by implementing them on robots. I am fortunate to be mentored by [Jeannette Bohg](https://web.stanford.edu/~bohg/), and previously by [Chelsea Finn](https://ai.stanford.edu/~cbfinn/), [Suraj Nair](https://suraj-nair-1.github.io/), and [Annie Xie](https://anxie.github.io/) of Stanford's [IRIS Lab](https://irislab.stanford.edu/).

My research interests span robotics, machine learning, and computer vision. I'm interested in enabling robots to *learn generalizable representations from diverse datasets*, and *refine them through interaction* for performing complex tasks in the real world. I'm especially interested in:
1. **Representation learning from diverse data**: Developing robust state and action representations for robot learning by extracting useful interaction priors from diverse human and robot data.
2. **Learning through interaction**: Enhancing precision and robustness of pre-trained policies via training in high-fidelity simulation and sample-efficient online fine-tuning.
3. **Autonomous data collection**: Enabling robots to continually acquire in-domain experience and practice skills with limited supervision.
<!-- 4. **Long-horizon planning**: Integrating robust skill learning with long-horizon task planning for adaptive, closed-loop task execution.--> 

If any of the above sounds interesting to you, I would love to hear from you! Feel free to reach me at oliviayl [at] stanford [dot] edu.

<!-- 10/21/2024
1. **Learning from human data**: Enabling robots to leverage skill and object representations learned from human data for downstream tasks.
2. **Long-horizon planning and reasoning**: Improving long-horizon task completion by processing multimodal inputs and environmental feedback.
3. **Representation learning**: Developing robust action and state representations for planning, goal specification, and closed-loop task execution.
-->

<!-- 6/24/2024 Hello! I am a final year undergraduate student at Stanford University (Class of 2024), pursuing a B.S. in [Symbolic Systems](https://symsys.stanford.edu/) with a minor in Mathematics and a coterminal M.S. in Computer Science. I conduct research with Stanford's [IRIS Lab](https://irislab.stanford.edu/) which studies intelligence through robotic interaction at scale, affiliated with the [Stanford Artificial Intelligence Laboratory (SAIL)](https://ai.stanford.edu/) and [Stanford Machine Learning Group](http://ml.stanford.edu/index.html). I am fortunate to be mentored by Professor [Chelsea Finn](https://ai.stanford.edu/~cbfinn/), [Suraj Nair](https://cs.stanford.edu/~surajn/), and [Annie Xie](https://anxie.github.io/).
1. **Visual pretraining and representation learning**: Enabling robots to harness skill and object representations for downstram tasks, learned from human data. 
2. **Interactive learning from multimodal data**: Facilitating human-compatible robot behaviors by enabling robots to process multimodal inputs and feedback.
3. **Continual data collection and learning**: Improving methods for continually acquiring in-domain data and skills with limited supervision for novel environment adaptation.  ->

<!-- 4/1/2024: 1. Visual pretraining and representation learning: I am excited by the potential of embodied agents learning skill and object representations via pretraining on large, diverse datasets, and using them for sample-efficient exploration or downstream tasks.
2. Interactive learning from multimodal human data: Humans communicate goals using various modalities, from language to physical corrections. I hope to facilitate human-compatible robot behaviors by enabling robots to process multimodal inputs and feedback, potentially leveraging large pretrained models.
3. Continual data collection and learning: Ideally robots should continually acquire experience and skills with limited supervision. I aim to improve autonomous exploration methods for scalably collecting in-domain robot data and adapting to novel environments.-->

<!-- 12/6/2023(2): 1. **Visual pretraining and representation learning**: In novel situations, humans don't re-learn skills and object representations from scratch. I am excited by the potential of robotic agents similarly learning environmental representations via pretraining on large, diverse datasets, and using these representations for exploration or downstream tasks. 
2. **Interactive learning from multimodal human data**: Humans communicate goals and provide feedback using various modalities, from language to physical corrections. I hope to develop methods capable of understanding multimodal task specifications to generate more expressive, human-compatible behaviors in robots, potentially leveraging Internet-scale multimodal pretrained models.
3. **Continual data collection and learning**: Supervision is costly, and I am excited by the prospect of robots continually acquiring knowledge and skills with limited supervision, as humans often do. Autonomous exploration also facilitates scalable, in-domain robot data collection, and I aim to improve learning from exploration for adaptively performing downstream tasks in novel environments. -->

<!-- 12/6/2023: More broadly, I'm interested in *embodied systems capable of intelligently exploring their environments, and harnessing learned knowledge for downstream tasks*. When faced with novel situations, humans don't re-learn skills and object representations from scratch. I am excited by the potential of robotic agents similarly learning representations of the environment, for instance via pretraining on (potentially multimodal) information, and using these representations to explore novel environments intelligently through interaction. After familiarizing itself with the specific objects and dynamics the new environment, the robot can then proceed with its assigned tasks. ... Through my current coursework and research, ... Some areas of human cognition that I hope to explore through a computational lens are multimodal perception, interactive learning, and curiosity.... engineer computational analogs of these processes in AI systems.-->


<!-- OLD: More broadly, I'm interested in embodied intelligent systems capable of learning quickly and flexibly by cooperating with humans. I am excited by the interplay between autonomous and interactive reinforcement learning: a robot should ideally operate and learn autonomously, but query a human operator upon recognizing it has reached an irreversible or unsafe state. By processing human information and feedback, potentially from multiple modalities (language, images, physical repositioning etc.), the robot can then proceed with its assigned tasks. -->