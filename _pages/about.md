---
permalink: /
title: "Welcome"
excerpt: "About me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

Hello! I am a final year undergraduate student at Stanford University (Class of 2024), pursuing a B.S. in [Symbolic Systems](https://symsys.stanford.edu/) with a minor in Mathematics and a coterminal M.S. in Computer Science. I conduct research with Stanford's [IRIS Lab](https://irislab.stanford.edu/) which studies intelligence through robotic interaction at scale, affiliated with the [Stanford Artificial Intelligence Laboratory (SAIL)](https://ai.stanford.edu/) and [Stanford Machine Learning Group](http://ml.stanford.edu/index.html). I am fortunate to be mentored by Professor [Chelsea Finn](https://ai.stanford.edu/~cbfinn/), [Suraj Nair](https://cs.stanford.edu/~surajn/), and [Annie Xie](https://anxie.github.io/).

My research interests span machine learning, robotics, and computer systems. Specifically, I'm interested in enabling robots to *learn generalizable representations from diverse datasets*, and *refine them through interaction* for performing complex tasks in the real world. I'm especially interested in the following:
1. **Visual pretraining and representation learning**: I am excited by the potential of embodied agents learning skill and object representations via pretraining on large, diverse datasets, and using them for sample-efficient exploration or downstream tasks. 
2. **Interactive learning from multimodal human data**: Humans communicate goals using various modalities, from language to physical corrections. I hope to facilitate human-compatible robot behaviors by enabling robots to process multimodal inputs and feedback, potentially leveraging large pretrained models.
3. **Continual data collection and learning**: Ideally robots should continually acquire experience and skills with limited supervision. I aim to improve autonomous exploration methods for scalably collecting in-domain robot data and adapting to novel environments. 

Inspired by my interdisciplinary coursework, I am drawn to research leveraging concepts in cognitive science for robot learning and visual understanding. I aim to better understand human cognitive processes, such as multimodal perception, curiosity, and interactive learning, to develop human-inspired learning algorithms for robotics. 

I am a U.S. citizen who grew up and was educated in Singapore. Besides research, I worked as a software engineer at Salesforce and at several startups, ranging from deep tech (quantum computing) to B2C companies based in Southeast Asia and the United States. I also studied abroad at the University of Oxford (Magdalen College) in Fall 2022, where I studied graph representation learning and philosophy of mind, and tried my hand at rowing! I also enjoy playing tennis, hiking, reading (+ occasionally writing) science fiction, and brush calligraphy.

If any of the above sounds interesting to you, I would love to hear from you! Feel free to reach me at oliviayl [at] stanford [dot] edu.


<!-- 12/6/2023(2): 1. **Visual pretraining and representation learning**: In novel situations, humans don't re-learn skills and object representations from scratch. I am excited by the potential of robotic agents similarly learning environmental representations via pretraining on large, diverse datasets, and using these representations for exploration or downstream tasks. 
2. **Interactive learning from multimodal human data**: Humans communicate goals and provide feedback using various modalities, from language to physical corrections. I hope to develop methods capable of understanding multimodal task specifications to generate more expressive, human-compatible behaviors in robots, potentially leveraging Internet-scale multimodal pretrained models.
3. **Continual data collection and learning**: Supervision is costly, and I am excited by the prospect of robots continually acquiring knowledge and skills with limited supervision, as humans often do. Autonomous exploration also facilitates scalable, in-domain robot data collection, and I aim to improve learning from exploration for adaptively performing downstream tasks in novel environments. -->

<!-- 12/6/2023: More broadly, I'm interested in *embodied systems capable of intelligently exploring their environments, and harnessing learned knowledge for downstream tasks*. When faced with novel situations, humans don't re-learn skills and object representations from scratch. I am excited by the potential of robotic agents similarly learning representations of the environment, for instance via pretraining on (potentially multimodal) information, and using these representations to explore novel environments intelligently through interaction. After familiarizing itself with the specific objects and dynamics the new environment, the robot can then proceed with its assigned tasks. ... Through my current coursework and research, ... Some areas of human cognition that I hope to explore through a computational lens are multimodal perception, interactive learning, and curiosity.... engineer computational analogs of these processes in AI systems.-->


<!-- OLD: More broadly, I'm interested in embodied intelligent systems capable of learning quickly and flexibly by cooperating with humans. I am excited by the interplay between autonomous and interactive reinforcement learning: a robot should ideally operate and learn autonomously, but query a human operator upon recognizing it has reached an irreversible or unsafe state. By processing human information and feedback, potentially from multiple modalities (language, images, physical repositioning etc.), the robot can then proceed with its assigned tasks. -->